Logging to wandb run sevixdd/decoder_train/120rz588-stellar-oath-2
Saving checkpoint locally
======================================== Loaded Config ========================================
Running training with 1 processes and NO distributed training
Training using clip image embeddings generation and clip text encoding generation. conditioned on text
Number of parameters: 108451334 total; 108451334 training
Unet 0 has 108451334 total; 108451334 training
======================================== Generating Example Data ========================================
This can take a while to load the shard lists...
Generated training examples
Generated testing examples
======================================== Starting epoch 0 ========================================
Traceback (most recent call last):
  File "/home/sevi/Documents/GitHub/FYP/train_decoder.py", line 650, in <module>
    main()
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/sevi/Documents/GitHub/FYP/train_decoder.py", line 647, in main
    initialize_training(config, config_path=config_file_path)
  File "/home/sevi/Documents/GitHub/FYP/train_decoder.py", line 632, in initialize_training
    train(dataloaders, decoder, accelerator,
  File "/home/sevi/Documents/GitHub/FYP/train_decoder.py", line 392, in train
    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/trainer.py", line 107, in inner
    out = fn(model, *args, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/trainer.py", line 723, in forward
    loss_obj = self.decoder(*chunked_args, unet_number = unet_number, return_lowres_cond_image=return_lowres_cond_image, **chunked_kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 3267, in forward
    losses = self.p_losses(unet, image, times, image_embed = image_embed, text_encodings = text_encodings, lowres_cond_img = lowres_cond_img, predict_x_start = predict_x_start, predict_v = predict_v, learned_variance = learned_variance, is_latent_diffusion = is_latent_diffusion, noise_scheduler = noise_scheduler, lowres_noise_level = lowres_noise_level)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 3048, in p_losses
    unet_output = unet(
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 2357, in forward
    x = final_resnet_block(x, t)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 1661, in forward
    return h + self.res_conv(x)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/sevi/anaconda3/envs/AdvAI/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 11.76 GiB total capacity; 9.18 GiB already allocated; 110.31 MiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
{'Epoch': 0, 'Sample': 10, 'Step': 0, 'Samples per second': 5.488980499042246, 'Samples Seen': 10, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 1.0003807544708252}