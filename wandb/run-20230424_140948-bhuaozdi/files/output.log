Logging to wandb run sevixdd/decoder_train/bhuaozdi-ethereal-firefly-12
Saving checkpoint locally
======================================== Loaded Config ========================================
Running training with 1 processes and NO distributed training
Training using clip image embeddings generation and clip text encoding generation. conditioned on text
Number of parameters: 280177542 total; 280177542 training
Unet 0 has 280177542 total; 280177542 training
======================================== Generating Example Data ========================================
This can take a while to load the shard lists...
Generated training examples
Generated testing examples
======================================== Starting epoch 0 ========================================
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 4, 'Step': 0, 'Samples per second': 2.4889057678613815, 'Samples Seen': 4, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 1.0000025033950806}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 44, 'Step': 10, 'Samples per second': 17.773867866760742, 'Samples Seen': 44, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9876346588134766}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 84, 'Step': 20, 'Samples per second': 17.899744795623118, 'Samples Seen': 84, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9343088269233704}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 124, 'Step': 30, 'Samples per second': 17.96849101747667, 'Samples Seen': 124, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.8554438352584839}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 164, 'Step': 40, 'Samples per second': 17.84510263168531, 'Samples Seen': 164, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.768525242805481}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 204, 'Step': 50, 'Samples per second': 17.961969549516564, 'Samples Seen': 204, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.6904070377349854}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 244, 'Step': 60, 'Samples per second': 17.849108513565128, 'Samples Seen': 244, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.5754572749137878}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 284, 'Step': 70, 'Samples per second': 17.893769311252868, 'Samples Seen': 284, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.509345293045044}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
Aborted!
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 324, 'Step': 80, 'Samples per second': 17.935604983461868, 'Samples Seen': 324, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.40301722288131714}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])