Logging to wandb run sevixdd/decoder_train/28tlyxp6-crisp-blaze-11
Saving checkpoint locally
======================================== Loaded Config ========================================
Running training with 1 processes and NO distributed training
Training using clip image embeddings generation and clip text encoding generation. conditioned on text
Number of parameters: 280177542 total; 280177542 training
Unet 0 has 280177542 total; 280177542 training
======================================== Generating Example Data ========================================
This can take a while to load the shard lists...
Generated training examples
Generated testing examples
======================================== Starting epoch 0 ========================================
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 4, 'Step': 0, 'Samples per second': 2.3913495749732676, 'Samples Seen': 4, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 1.0000025033950806}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 44, 'Step': 10, 'Samples per second': 17.864712509290552, 'Samples Seen': 44, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9876455068588257}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 84, 'Step': 20, 'Samples per second': 17.963335010760517, 'Samples Seen': 84, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9346494674682617}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 124, 'Step': 30, 'Samples per second': 17.872001176040726, 'Samples Seen': 124, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.8571218252182007}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 164, 'Step': 40, 'Samples per second': 17.986213284561668, 'Samples Seen': 164, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.7740136981010437}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 204, 'Step': 50, 'Samples per second': 17.938961057013234, 'Samples Seen': 204, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.6993803977966309}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 244, 'Step': 60, 'Samples per second': 17.90994860977375, 'Samples Seen': 244, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.5880904197692871}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
Aborted!
torch.Size([4, 77])
{'Epoch': 0, 'Sample': 284, 'Step': 70, 'Samples per second': 17.91455751281616, 'Samples Seen': 284, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.5152223706245422}
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
torch.Size([4, 77])
