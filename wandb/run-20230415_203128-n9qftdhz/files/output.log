Logging to wandb run sevixdd/decoder_train/n9qftdhz-whole-armadillo-7
Saving checkpoint locally
======================================== Loaded Config ========================================
Running training with 1 processes and NO distributed training
Training using clip image embeddings generation and clip text encoding generation. conditioned on text
Number of parameters: 280177542 total; 280177542 training
Unet 0 has 280177542 total; 280177542 training
======================================== Generating Example Data ========================================
This can take a while to load the shard lists...
Generated training examples
Generated testing examples
======================================== Starting epoch 0 ========================================
{'Epoch': 0, 'Sample': 4, 'Step': 0, 'Samples per second': 2.510916516879661, 'Samples Seen': 4, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 1.0000025033950806}
{'Epoch': 0, 'Sample': 44, 'Step': 10, 'Samples per second': 18.035110878438445, 'Samples Seen': 44, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9875626564025879}
{'Epoch': 0, 'Sample': 84, 'Step': 20, 'Samples per second': 18.0992386917624, 'Samples Seen': 84, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.934634804725647}
{'Epoch': 0, 'Sample': 124, 'Step': 30, 'Samples per second': 18.067312445145625, 'Samples Seen': 124, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.854739785194397}
{'Epoch': 0, 'Sample': 164, 'Step': 40, 'Samples per second': 18.027669220114824, 'Samples Seen': 164, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.7675436735153198}
Aborted!