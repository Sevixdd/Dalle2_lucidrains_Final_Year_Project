Logging to wandb run sevixdd/decoder_train/2z47qnzu-flowing-fog-1
Saving checkpoint locally
======================================== Loaded Config ========================================
Running training with 1 processes and NO distributed training
Training using clip image embeddings generation and clip text encoding generation. conditioned on text
Number of parameters: 108451334 total; 108451334 training
Unet 0 has 108451334 total; 108451334 training
======================================== Generating Example Data ========================================
This can take a while to load the shard lists...
Generated training examples
Generated testing examples
======================================== Starting epoch 0 ========================================
{'Epoch': 0, 'Sample': 8, 'Step': 0, 'Samples per second': 4.018527576032954, 'Samples Seen': 8, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 1.0006802082061768}
{'Epoch': 0, 'Sample': 88, 'Step': 10, 'Samples per second': 29.096322554519134, 'Samples Seen': 88, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9863382577896118}
{'Epoch': 0, 'Sample': 168, 'Step': 20, 'Samples per second': 29.139758575770735, 'Samples Seen': 168, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.9306909441947937}
{'Epoch': 0, 'Sample': 248, 'Step': 30, 'Samples per second': 28.918431492192195, 'Samples Seen': 248, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.8468648791313171}
{'Epoch': 0, 'Sample': 328, 'Step': 40, 'Samples per second': 29.034689058353315, 'Samples Seen': 328, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.748612105846405}
{'Epoch': 0, 'Sample': 408, 'Step': 50, 'Samples per second': 28.967488181444608, 'Samples Seen': 408, 'Unet 0 EMA Decay': 0.0, 'Unet 0 Training Loss': 0.6525514721870422}
Aborted!
